{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a24574f-810d-48f5-b6cf-92297cd21e30",
   "metadata": {},
   "source": [
    "# Lab 4. Retrieval-augmented generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e247ed1a-00a6-4005-b362-8dcb7c0eaa19",
   "metadata": {},
   "source": [
    "## 1. Basic Completion and Chat\n",
    "### Download Qwen2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d18203a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from modelscope import snapshot_download\n",
    "llm_model_id = \"snake7gun/Qwen2-7B-Instruct-int4-ov\"\n",
    "llm_local_path  = \"./model/snake7gun/Qwen2-7B-Instruct-int4-ov\"\n",
    "\n",
    "if not Path(llm_local_path).exists():\n",
    "    model_dir = snapshot_download(llm_model_id, cache_dir=\"./model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7928626d",
   "metadata": {},
   "source": [
    "### Initialize LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a75c915-d4fc-4016-89c6-dba61edea9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\intel123\\miniforge3\\lib\\site-packages\\pydantic\\_internal\\_fields.py:161: UserWarning: Field \"model_id\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "Compiling the model to GPU ...\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openvino import OpenVINOLLM\n",
    "\n",
    "ov_config = {\n",
    "    \"PERFORMANCE_HINT\": \"LATENCY\",\n",
    "    \"NUM_STREAMS\": \"1\",\n",
    "    \"CACHE_DIR\": \"\",\n",
    "}\n",
    "\n",
    "def completion_to_prompt(completion):\n",
    "   return f\"<|im_start|>system\\n<|im_end|>\\n<|im_start|>user\\n{completion}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "def messages_to_prompt(messages):\n",
    "    prompt = \"\"\n",
    "    for message in messages:\n",
    "        if message.role == \"system\":\n",
    "            prompt += f\"<|im_start|>system\\n{message.content}<|im_end|>\\n\"\n",
    "        elif message.role == \"user\":\n",
    "            prompt += f\"<|im_start|>user\\n{message.content}<|im_end|>\\n\"\n",
    "        elif message.role == \"assistant\":\n",
    "            prompt += f\"<|im_start|>assistant\\n{message.content}<|im_end|>\\n\"\n",
    "\n",
    "    if not prompt.startswith(\"<|im_start|>system\"):\n",
    "        prompt = \"<|im_start|>system\\n\" + prompt\n",
    "\n",
    "    prompt = prompt + \"<|im_start|>assistant\\n\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "ov_llm = OpenVINOLLM(\n",
    "    model_id_or_path=llm_local_path,\n",
    "    context_window=3900,\n",
    "    max_new_tokens=1024,\n",
    "    model_kwargs={\"ov_config\": ov_config},\n",
    "    generate_kwargs={\"pad_token_id\": 32000, \"do_sample\": False, \"temperature\": None, \"top_p\": None},\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    device_map=\"gpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d6719d-299c-4a11-9f0a-b2ef2d5db716",
   "metadata": {},
   "source": [
    "### Call complete with a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9aec4ad-efd1-4a56-9343-e8d7cc61ab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ov_llm.stream_complete(\"What is OpenVINO ?\")\n",
    "\n",
    "for r in response:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c6cdf6-4358-43aa-a380-2c315f36c228",
   "metadata": {},
   "source": [
    "## 2. Basic RAG (Vector Search, Summarization)\n",
    "### Export Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "536ea7ff-033b-4a53-ba2c-6169c69e47d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Library name is not specified. There are multiple possible variants: `sentence_transformers`, `transformers`.`transformers` will be selected. If you want to load your model with the `sentence-transformers` library instead, please set --library sentence_transformers\n",
      "Framework not specified. Using pt to export the model.\n",
      "C:\\Users\\intel123\\miniconda3\\envs\\openvino_aipc\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\intel123\\.cache\\huggingface\\hub\\models--BAAI--bge-small-zh-v1.5. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Using framework PyTorch: 2.3.1+cpu\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n",
      "Detokenizer is not supported, convert tokenizer only.\n"
     ]
    }
   ],
   "source": [
    "embedding_model_id = \"BAAI/bge-small-zh-v1.5\"\n",
    "embedding_model_path = \"./model/bge-small-zh-v1.5-ov\"\n",
    "\n",
    "if not Path(embedding_model_path).exists():\n",
    "    !optimum-cli export openvino --model {embedding_model_id} --task feature-extraction {embedding_model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f82691",
   "metadata": {},
   "source": [
    "### Initialize Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7a768ac-b701-4722-8d88-f1d2561feb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling the model to CPU ...\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface_openvino import OpenVINOEmbedding\n",
    "\n",
    "ov_embedding = OpenVINOEmbedding(model_id_or_path=embedding_model_path, device=\"CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84ace02-5bec-4815-a709-0b6423497516",
   "metadata": {},
   "source": [
    "### Basic RAG (Vector Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed13be16-0364-445e-acc9-d0b466411d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core import VectorStoreIndex, Settings\n",
    "\n",
    "Settings.embed_model = ov_embedding\n",
    "Settings.llm = ov_llm\n",
    "\n",
    "reader = SimpleDirectoryReader(\n",
    "    input_files=[\"./examples/text_example_cn.pdf\"]\n",
    ")\n",
    "documents = reader.load_data()\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    ")\n",
    "query_engine = index.as_query_engine(streaming=True, similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b492fb4-7667-492b-b8f8-0048fc2b592b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英特尔博锐® Enterprise系统提供了以下功能：\n",
      "\n",
      "1. 动态信任根\n",
      "2. 系统管理模式（SMM）保护\n",
      "3. 具有多密钥支持的内存加密\n",
      "4. 操作系统内核保护\n",
      "5. 可实现远程KVM控制的带外管理\n",
      "6. 唯一设备标识符\n",
      "7. 设备历史记录\n",
      "8. 带内可管理性插件"
     ]
    }
   ],
   "source": [
    "streaming_response = query_engine.query(\"英特尔博锐® Enterprise系统提供哪些功能?\")\n",
    "streaming_response.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ef4fa99-abaf-40ad-84f8-61d714802d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "相比英特尔之前的移动处理器产品，英特尔®酷睿™ Ultra处理器的每瓦AI推理性能最高提升了2.5倍。"
     ]
    }
   ],
   "source": [
    "streaming_response = query_engine.query(\"相比英特尔之前的移动处理器产品，英特尔®酷睿™ Ultra处理器的AI推理性能提升了多少？\")\n",
    "streaming_response.print_response_stream()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
